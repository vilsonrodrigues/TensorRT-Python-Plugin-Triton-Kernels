{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNHgTtCLMkqSGmSp7zjPBac",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vilsonrodrigues/TensorRT-Python-Plugin-Triton-Kernels/blob/main/tensorrt_python_plugin_triton_kernel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://towardsdatascience.com/writing-custom-cuda-kernels-with-triton-abf6b6ad1168\n",
        "# https://ai.lefebvre-sarrut.eu/2023/07/20/deep-dive-into-kernel-fusion-accelerating-inference-in-llama-v2/#openai-triton-rewriting"
      ],
      "metadata": {
        "id": "XmATuUX-QioX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OjjDV7rY86M",
        "outputId": "cf479f48-a518-4d4a-b161-2dae3af72980"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jul 22 21:42:34 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   48C    P8              13W /  72W |      1MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torch tensorrt cuda-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDDOrddbSLUN",
        "outputId": "387ddb6d-b188-43db-dd9b-f0d4da42b706"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Collecting tensorrt\n",
            "  Downloading tensorrt-10.2.0.post1.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cuda-python in /usr/local/lib/python3.10/dist-packages (12.2.1)\n",
            "Collecting cuda-python\n",
            "  Downloading cuda_python-12.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.8/24.8 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorrt-cu12==10.2.0.post1 (from tensorrt)\n",
            "  Downloading tensorrt-cu12-10.2.0.post1.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Building wheels for collected packages: tensorrt, tensorrt-cu12\n",
            "  Building wheel for tensorrt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt: filename=tensorrt-10.2.0.post1-py2.py3-none-any.whl size=16418 sha256=fd1b7d03f799ac3994a3d41dd99f17dfe11172d3e17dd54a28fff381230090df\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/5f/d0/e0184ce80b75930f1376ad398ee9e08a45861d0057ee358e2d\n",
            "  Building wheel for tensorrt-cu12 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt-cu12: filename=tensorrt_cu12-10.2.0.post1-py2.py3-none-any.whl size=17631 sha256=2e614c414f69dcd0b34958773538025ee09ea6165cf1abdeb97e88f1e3f85003\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/be/a0/0708e6c9c1b108c54cbe0425eb84c7e9a49b5e1d31bb2115fd\n",
            "Successfully built tensorrt tensorrt-cu12\n",
            "Installing collected packages: cuda-python, tensorrt-cu12, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tensorrt, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: cuda-python\n",
            "    Found existing installation: cuda-python 12.2.1\n",
            "    Uninstalling cuda-python-12.2.1:\n",
            "      Successfully uninstalled cuda-python-12.2.1\n",
            "Successfully installed cuda-python-12.5.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 tensorrt-10.2.0.post1 tensorrt-cu12-10.2.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx onnx_graphsurgeon"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfTJjm11ZnAH",
        "outputId": "6f33fc09-44c7-47dc-f2a4-5f3ead539226"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnx_graphsurgeon\n",
            "  Downloading onnx_graphsurgeon-0.5.2-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Installing collected packages: onnx, onnx_graphsurgeon\n",
            "Successfully installed onnx-1.16.1 onnx_graphsurgeon-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0lDMSaBBIn-",
        "outputId": "1973842f-2b0c-4f60-db61-e2af719dfd68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input ->DataType.FLOAT, (1, -1), (1, 3), A\n",
            "Input ->DataType.FLOAT, (1, -1), (1, 3), B\n",
            "Output->DataType.FLOAT, (1, -1), (1, 3), C\n",
            "A\n",
            "tensor([[1., 1., 1.]], device='cuda:0')\n",
            "B\n",
            "tensor([[1., 1., 1.]], device='cuda:0')\n",
            "C\n",
            "tensor([[2., 2., 2.]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "import ctypes\n",
        "from typing import List\n",
        "import numpy as np\n",
        "import tensorrt as trt\n",
        "import torch\n",
        "from cuda import cudart\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "@triton.jit\n",
        "def add_kernel(x_ptr,  # *Pointer* to first input vector.\n",
        "               y_ptr,  # *Pointer* to second input vector.\n",
        "               output_ptr,  # *Pointer* to output vector.\n",
        "               n_elements,  # Size of the vector.\n",
        "               BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.\n",
        "               # NOTE: `constexpr` so it can be used as a shape value.\n",
        "               ):\n",
        "    # There are multiple 'programs' processing different data. We identify which program\n",
        "    # we are here:\n",
        "    pid = tl.program_id(0)  # tl.program_id(0) -> blockIdx.x\n",
        "    # This program will process inputs that are offset from the initial data.\n",
        "    # For instance, if you had a vector of length 256 and block_size of 64, the programs\n",
        "    # would each access the elements [0:64, 64:128, 128:192, 192:256].\n",
        "    # Note that offsets is a list of pointers:\n",
        "    block_start = pid * BLOCK_SIZE\n",
        "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
        "    # Create a mask to guard memory operations against out-of-bounds accesses.\n",
        "    mask = offsets < n_elements\n",
        "    # Load x and y from DRAM, masking out any extra elements in case the input is not a\n",
        "    # multiple of the block size.\n",
        "    x = tl.load(x_ptr + offsets, mask=mask)\n",
        "    y = tl.load(y_ptr + offsets, mask=mask)\n",
        "    output = x + y\n",
        "    # Write x + y back to DRAM.\n",
        "    tl.store(output_ptr + offsets, output, mask=mask)\n",
        "\n",
        "# Let's also declare a helper function to (1) allocate the `c` tensor\n",
        "# and (2) enqueue the above kernel with appropriate grid/block sizes:\n",
        "def add_layer(a: torch.Tensor, b: torch.Tensor):\n",
        "    # We need to preallocate the output.\n",
        "    output = torch.empty_like(a)\n",
        "    assert a.is_cuda and b.is_cuda and output.is_cuda\n",
        "    n_elements = output.numel()\n",
        "    # The SPMD launch grid denotes the number of kernel instances that run in parallel.\n",
        "    # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].\n",
        "    # In this case, we use a 1D grid where the size is the number of blocks:\n",
        "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )\n",
        "    # NOTE:\n",
        "    #  - Each torch.tensor object is implicitly converted into a pointer to its first element.\n",
        "    #  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel.\n",
        "    #  - Don't forget to pass meta-parameters as keywords arguments.\n",
        "    add_kernel[grid](a, b, output, n_elements, BLOCK_SIZE=256)\n",
        "    # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still\n",
        "    # running asynchronously at this point.\n",
        "    return output\n",
        "\n",
        "class AddPlugin(trt.IPluginV3, trt.IPluginV3OneCore, trt.IPluginV3OneBuild, trt.IPluginV3OneRuntime):\n",
        "\n",
        "    def __init__(self, field_collections=None):\n",
        "        trt.IPluginV3.__init__(self)\n",
        "        trt.IPluginV3OneCore.__init__(self)\n",
        "        trt.IPluginV3OneBuild.__init__(self)\n",
        "        trt.IPluginV3OneRuntime.__init__(self)\n",
        "        self.plugin_name = \"AddPlugin\"  # necessary as function `getPluginType` in C++\n",
        "        self.plugin_version = \"1\"  # necessary as function `getPluginVersion` in C++\n",
        "        self.num_outputs = 1  # necessary as function `getNbOutputs` in C++\n",
        "        self.plugin_namespace = \"\"  # necessary as function `setPluginNamespace`/ `getPluginNamespace` in C++\n",
        "        self.device = 0  # default device is cuda:0, can be get by `cuda.cuDeviceGet(0)`\n",
        "        return\n",
        "\n",
        "    def get_capability_interface(self, plugin_capability_type: trt.PluginCapabilityType) -> trt.IPluginCapability:\n",
        "        return self\n",
        "\n",
        "    def clone(self) -> trt.IPluginV3:\n",
        "        cloned_plugin = AddPlugin()\n",
        "        cloned_plugin.__dict__.update(self.__dict__)\n",
        "        return cloned_plugin\n",
        "\n",
        "    def configure_plugin(self, dptd_in: List[trt.DynamicPluginTensorDesc], dptd_out: List[trt.DynamicPluginTensorDesc]) -> None:\n",
        "        \"\"\" Configure the plugin based-on inputs and outputs\n",
        "\n",
        "        No required to this plugin\n",
        "        Example: https://github.com/NVIDIA/TensorRT/blob/release/10.2/samples/python/python_plugin/circ_pad_plugin_triton.py#L125\n",
        "        \"\"\"\n",
        "        return\n",
        "\n",
        "    def get_output_data_types(self, input_types: List[trt.DataType]) -> List[trt.DataType]:\n",
        "        \"\"\" Describe the output data type based-on inputs\n",
        "\n",
        "        In this plugin the output data type is the same than input because\n",
        "        add op no change the data type\n",
        "        \"\"\"\n",
        "        return [input_types[0]]\n",
        "\n",
        "    def get_output_shapes(self, inputs: List[trt.DimsExprs], shape_inputs: List[trt.DimsExprs], expr_builder: trt.IExprBuilder) -> List[trt.DimsExprs]:\n",
        "        \"\"\" Describe the output shape based-on inputs\n",
        "\n",
        "        In this plugin the output is the same than input because\n",
        "        add op no change the shape\n",
        "        \"\"\"\n",
        "        output_dims = trt.DimsExprs(inputs[0])\n",
        "        return [output_dims]\n",
        "\n",
        "    def supports_format_combination(self, pos: int, in_out: List[trt.DynamicPluginTensorDesc], num_inputs: int) -> bool:\n",
        "        \"\"\" Check if the data format is supported by plugin\n",
        "\n",
        "        Args:\n",
        "            pos: tensor position (in/out)\n",
        "            in_out: dynamic tensor description\n",
        "            num_inputs: number of inputs\n",
        "\n",
        "        Returns:\n",
        "            True if supported, False if not\n",
        "        \"\"\"\n",
        "        assert num_inputs > 1\n",
        "        assert pos < len(in_out)\n",
        "\n",
        "        res = False\n",
        "        desc = in_out[pos].desc\n",
        "        # check inputs\n",
        "        if pos in [0, 1]:\n",
        "            # inputs should be float16 or float32\n",
        "            res = (desc.type == trt.float32 or desc.type == trt.float16) and desc.format == trt.TensorFormat.LINEAR\n",
        "        # check output\n",
        "        elif pos == 2:\n",
        "            # output should have the same type as the input\n",
        "            res = desc.type == in_out[0].desc.type and desc.format == trt.TensorFormat.LINEAR\n",
        "        if False:  # print information about the input / output\n",
        "            info = f\"    {pos=}:\"\n",
        "            info += f\"{[str(in_out[i].type)[9:] for i in range(len(in_out))]},\"\n",
        "            info += f\"{[str(in_out[i].format)[13:] for i in range(len(in_out))]}\"\n",
        "            info += f\"->{res=}\"\n",
        "            print(info)\n",
        "        return res\n",
        "\n",
        "    def get_workspace_size(self, dptd_in: List[trt.DynamicPluginTensorDesc], dptd_out: List[trt.DynamicPluginTensorDesc]) -> int:\n",
        "        \"\"\" Return necessary workspace to this plugin \"\"\"\n",
        "        return 0\n",
        "\n",
        "    def get_valid_tactics(self) -> List[int]:\n",
        "        \"\"\" Return a list from valid tatics \"\"\"\n",
        "        return [1]\n",
        "\n",
        "    def set_tactic(self: trt.IPluginV3, tactic: int) -> None:\n",
        "        \"\"\" Define a tatic to be used to this plugin \"\"\"\n",
        "        return None\n",
        "\n",
        "    def on_shape_change(self, ptd_in: List[trt.PluginTensorDesc], ptd_out: List[trt.PluginTensorDesc]) -> ModuleNotFoundError:\n",
        "        \"\"\" Method called when there is a chance in tensors \"\"\"\n",
        "        return None\n",
        "\n",
        "    def attach_to_context(self, resource_context: trt.IPluginResourceContext) -> trt.IPluginV3:\n",
        "        \"\"\" Attach this plugin in a context \"\"\"\n",
        "        return self.clone()\n",
        "\n",
        "    def get_fields_to_serialize(self) -> trt.PluginFieldCollection:\n",
        "        \"\"\" Returns the fields to serialize\n",
        "\n",
        "        To this plugin no field is implemented\n",
        "        \"\"\"\n",
        "        field_collection = trt.PluginFieldCollection([])\n",
        "        return field_collection\n",
        "\n",
        "    def enqueue(self, input_desc: List[trt.PluginTensorDesc], output_desc: List[trt.PluginTensorDesc], inputs: List[int], outputs: List[int], workspace: int, stream: int) -> None:\n",
        "        \"\"\" Execute kernel on GPU\n",
        "\n",
        "        Implement C = A + B using OpenAI Triton\n",
        "\n",
        "        Args:\n",
        "            input_desc: input tensor description list\n",
        "            output_desc: output tensor description list\n",
        "            inputs: pointer list to inputs\n",
        "            outputs: pointer list to outputs\n",
        "            workspace\n",
        "            stream\n",
        "        \"\"\"\n",
        "        A_DATA_TYPE = trt.nptype(input_desc[0].type)\n",
        "        B_DATA_TYPE = trt.nptype(input_desc[1].type)\n",
        "        assert A_DATA_TYPE == B_DATA_TYPE\n",
        "\n",
        "        A_SHAPE = tuple(input_desc[0].dims)\n",
        "        B_SHAPE = tuple(input_desc[1].dims)\n",
        "        assert A_SHAPE == B_SHAPE\n",
        "\n",
        "        A_N_ELEMENT = np.prod(A_SHAPE)\n",
        "        B_N_ELEMENT = np.prod(B_SHAPE)\n",
        "\n",
        "        # pointer -> numpy.ndarray -> torch.Tensor\n",
        "        BUFFER_SIZE = A_N_ELEMENT * np.dtype(A_DATA_TYPE).itemsize\n",
        "        cpp_data_type = ctypes.c_int16 if A_DATA_TYPE == np.float16 else ctypes.c_float\n",
        "\n",
        "        A_POINTER = ctypes.cast(inputs[0], ctypes.POINTER(cpp_data_type * A_N_ELEMENT))[0]\n",
        "        A_NP_ARRAY = np.ndarray(A_SHAPE, dtype=A_DATA_TYPE, buffer=A_POINTER)\n",
        "        A_TENSOR = torch.as_tensor(A_NP_ARRAY, device=\"cuda\")\n",
        "\n",
        "        B_POINTER = ctypes.cast(inputs[1], ctypes.POINTER(cpp_data_type * B_N_ELEMENT))[0]\n",
        "        B_NP_ARRAY = np.ndarray(B_SHAPE, dtype=B_DATA_TYPE, buffer=B_POINTER)\n",
        "        B_TENSOR = torch.as_tensor(B_NP_ARRAY, device=\"cuda\")\n",
        "\n",
        "        # C = A + B with triton add layer\n",
        "        C_TENSOR = add_layer(A_TENSOR, B_TENSOR)\n",
        "\n",
        "        # copy add output to buffer\n",
        "        kind = cudart.cudaMemcpyKind.cudaMemcpyDeviceToDevice\n",
        "        cudart.cudaMemcpyAsync(outputs[0], C_TENSOR.data_ptr(), BUFFER_SIZE, kind, stream)\n",
        "        return\n",
        "\n",
        "class AddPluginCreator(trt.IPluginCreatorV3One):\n",
        "\n",
        "    def __init__(self):\n",
        "        trt.IPluginCreatorV3One.__init__(self)\n",
        "        self.name = \"AddPlugin\"  # necessary as function `getPluginName` in C++\n",
        "        self.plugin_version = \"1\"  # necessary as function `getPluginVersion` in C++\n",
        "        self.plugin_namespace = \"\"  # necessary as function `setPluginNamespace`/ `getPluginNamespace` in C++\n",
        "        self.field_names = trt.PluginFieldCollection([])\n",
        "        return\n",
        "\n",
        "    def create_plugin(self, name: str, field_collection: trt.PluginFieldCollection, phase: trt.TensorRTPhase):\n",
        "        return AddPlugin()\n",
        "\n",
        "def datatype_np_to_torch(datatype_np):\n",
        "    if datatype_np == np.float32:\n",
        "        return torch.float32\n",
        "    if datatype_np == np.float16:\n",
        "        return torch.float16\n",
        "    if datatype_np == np.int8:\n",
        "        return torch.int8\n",
        "    if datatype_np == np.int32:\n",
        "        return torch.int32\n",
        "    if datatype_np == bool:\n",
        "        return torch.bool\n",
        "    if datatype_np == np.uint8:\n",
        "        return torch.uint8\n",
        "    if datatype_np == np.int64:\n",
        "        return torch.int64\n",
        "    return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from collections import OrderedDict\n",
        "    import onnx\n",
        "    import onnx_graphsurgeon as gs\n",
        "    import numpy as np\n",
        "\n",
        "    onnx_path = \"add_model.onnx\"\n",
        "\n",
        "    # create input variables with batch fixed = 1\n",
        "    input_A = gs.Variable(name=\"A\", dtype=np.float32, shape=(1, \"N\"))\n",
        "    input_B = gs.Variable(name=\"B\", dtype=np.float32, shape=(1, \"N\"))\n",
        "\n",
        "    # create output variable\n",
        "    output = gs.Variable(name=\"C\", dtype=np.float32)\n",
        "\n",
        "    # create node with custom plugin op\n",
        "    add_plugin_node = gs.Node(\n",
        "        name=\"AddPlugin\",\n",
        "        op=\"AddPlugin\",\n",
        "        inputs=[input_A, input_B],\n",
        "        outputs=[output],\n",
        "        #attrs={\"pads\": pads},\n",
        "    )\n",
        "\n",
        "    graph = gs.Graph(nodes=[add_plugin_node], inputs=[input_A, input_B], outputs=[output], opset=17)\n",
        "    onnx.save(gs.export_onnx(graph), onnx_path)\n",
        "\n",
        "    # build engine\n",
        "    TRT_LOGGER = trt.Logger(trt.Logger.VERBOSE)\n",
        "\n",
        "    # register plugin creator\n",
        "    plg_registry = trt.get_plugin_registry()\n",
        "    my_plugin_creator = AddPluginCreator()\n",
        "    plg_registry.register_creator(my_plugin_creator, \"\")\n",
        "\n",
        "    builder = trt.Builder(TRT_LOGGER)\n",
        "    config = builder.create_builder_config()\n",
        "    # Set cache\n",
        "    cache = config.create_timing_cache(b\"\")\n",
        "    config.set_timing_cache(cache, ignore_mismatch=False)\n",
        "    flag = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
        "    network = builder.create_network(flag)\n",
        "    parser = trt.OnnxParser(network, TRT_LOGGER)\n",
        "    with open(onnx_path, \"rb\") as f:\n",
        "        if not parser.parse(f.read()):\n",
        "            print(f\"ERROR: Failed to parse the ONNX file {onnx_path}\")\n",
        "            for error in range(parser.num_errors):\n",
        "                print(parser.get_error(error))\n",
        "    inputs = [network.get_input(i) for i in range(network.num_inputs)]\n",
        "    outputs = [network.get_output(i) for i in range(network.num_outputs)]\n",
        "\n",
        "    # set opt profile\n",
        "    profile = builder.create_optimization_profile()\n",
        "    # define range\n",
        "    min_shape = [1] + [1]\n",
        "    opt_shape = [1] + [1000]\n",
        "    max_shape = [1] + [10000]\n",
        "    for input in inputs:\n",
        "        profile.set_shape(input.name, min_shape, opt_shape, max_shape)\n",
        "    config.add_optimization_profile(profile)\n",
        "\n",
        "    engine_bytes = builder.build_serialized_network(network, config)\n",
        "    engine_path = \"add_model.engine\"\n",
        "    with open(engine_path, \"wb\") as f:\n",
        "        f.write(engine_bytes)\n",
        "\n",
        "    runtime = trt.Runtime(TRT_LOGGER)\n",
        "    with open(engine_path, \"rb\") as plan:\n",
        "        engine = runtime.deserialize_cuda_engine(plan.read())\n",
        "\n",
        "    # define inputs\n",
        "    A = np.ones((1, 3))\n",
        "    B = np.ones((1, 3))\n",
        "\n",
        "    # create stream\n",
        "    err, stream = cudart.cudaStreamCreate()\n",
        "\n",
        "    # create Execution Context from the engine (analogy to a GPU context, or a CPU process)\n",
        "    context = engine.create_execution_context()\n",
        "\n",
        "    # get i/o tensors names\n",
        "    tensor_name_list = [engine.get_tensor_name(i) for i in range(engine.num_io_tensors)]\n",
        "\n",
        "    # set runtime size of input tensor if using Dynamic-Shape mode\n",
        "    context.set_input_shape(tensor_name_list[0], A.shape)\n",
        "    context.set_input_shape(tensor_name_list[1], B.shape)\n",
        "\n",
        "    # Print information of input / output tensors\n",
        "    for name in tensor_name_list:\n",
        "        mode = engine.get_tensor_mode(name)\n",
        "        data_type = engine.get_tensor_dtype(name)\n",
        "        buildtime_shape = engine.get_tensor_shape(name)\n",
        "        runtime_shape = context.get_tensor_shape(name)\n",
        "        print(f\"{'Input ' if mode == trt.TensorIOMode.INPUT else 'Output'}->{data_type}, {buildtime_shape}, {runtime_shape}, {name}\")\n",
        "\n",
        "    device = \"cuda:0\"\n",
        "    # prepare the memory buffer on host and device\n",
        "    buffer = OrderedDict()\n",
        "    for name in tensor_name_list:\n",
        "        data_type = engine.get_tensor_dtype(name)\n",
        "        runtime_shape = context.get_tensor_shape(name)\n",
        "        buffer[name] = torch.empty(tuple(runtime_shape), dtype=datatype_np_to_torch(data_type), device=device)\n",
        "\n",
        "    # model input\n",
        "    data_A = torch.ones((1,3), dtype=torch.float32)\n",
        "    data_B = torch.ones((1,3), dtype=torch.float32)\n",
        "\n",
        "    # set runtime data, MUST use contiguous, it is a SERIOUS lesson\n",
        "    buffer[tensor_name_list[0]] = data_A.contiguous().to(device)\n",
        "    buffer[tensor_name_list[1]] = data_B.contiguous().to(device)\n",
        "\n",
        "    for name in tensor_name_list:\n",
        "        # bind address of buffer to context\n",
        "        context.set_tensor_address(name, buffer[name].data_ptr())\n",
        "\n",
        "    # do inference computation\n",
        "    context.execute_async_v3(stream)\n",
        "\n",
        "    # synchronize to get outputs\n",
        "    cudart.cudaStreamSynchronize(stream)\n",
        "    for name in tensor_name_list:\n",
        "        print(name)\n",
        "        print(buffer[name])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZCETRzLUP_t",
        "outputId": "5c0b3a8e-93fa-4399-b564-0a520680297e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "absl-py==1.4.0\n",
            "accelerate==0.32.1\n",
            "aiohttp==3.9.5\n",
            "aiosignal==1.3.1\n",
            "alabaster==0.7.16\n",
            "albumentations==1.3.1\n",
            "altair==4.2.2\n",
            "annotated-types==0.7.0\n",
            "anyio==3.7.1\n",
            "argon2-cffi==23.1.0\n",
            "argon2-cffi-bindings==21.2.0\n",
            "array_record==0.5.1\n",
            "arviz==0.15.1\n",
            "astropy==5.3.4\n",
            "astunparse==1.6.3\n",
            "async-timeout==4.0.3\n",
            "atpublic==4.1.0\n",
            "attrs==23.2.0\n",
            "audioread==3.0.1\n",
            "autograd==1.6.2\n",
            "Babel==2.15.0\n",
            "backcall==0.2.0\n",
            "beautifulsoup4==4.12.3\n",
            "bidict==0.23.1\n",
            "bigframes==1.11.1\n",
            "bleach==6.1.0\n",
            "blinker==1.4\n",
            "blis==0.7.11\n",
            "blosc2==2.0.0\n",
            "bokeh==3.3.4\n",
            "bqplot==0.12.43\n",
            "branca==0.7.2\n",
            "build==1.2.1\n",
            "CacheControl==0.14.0\n",
            "cachetools==5.4.0\n",
            "catalogue==2.0.10\n",
            "certifi==2024.7.4\n",
            "cffi==1.16.0\n",
            "chardet==5.2.0\n",
            "charset-normalizer==3.3.2\n",
            "chex==0.1.86\n",
            "click==8.1.7\n",
            "click-plugins==1.1.1\n",
            "cligj==0.7.2\n",
            "cloudpathlib==0.18.1\n",
            "cloudpickle==2.2.1\n",
            "cmake==3.27.9\n",
            "cmdstanpy==1.2.4\n",
            "colorcet==3.1.0\n",
            "colorlover==0.3.0\n",
            "colour==0.1.5\n",
            "community==1.0.0b1\n",
            "confection==0.1.5\n",
            "cons==0.4.6\n",
            "contextlib2==21.6.0\n",
            "contourpy==1.2.1\n",
            "cryptography==42.0.8\n",
            "cuda-python==12.5.0\n",
            "cudf-cu12 @ https://pypi.nvidia.com/cudf-cu12/cudf_cu12-24.4.1-cp310-cp310-manylinux_2_28_x86_64.whl#sha256=57366e7ef09dc63e0b389aff20df6c37d91e2790065861ee31a4720149f5b694\n",
            "cufflinks==0.17.3\n",
            "cupy-cuda12x==12.2.0\n",
            "cvxopt==1.3.2\n",
            "cvxpy==1.3.4\n",
            "cycler==0.12.1\n",
            "cymem==2.0.8\n",
            "Cython==3.0.10\n",
            "dask==2023.8.1\n",
            "datascience==0.17.6\n",
            "db-dtypes==1.2.0\n",
            "dbus-python==1.2.18\n",
            "debugpy==1.6.6\n",
            "decorator==4.4.2\n",
            "defusedxml==0.7.1\n",
            "distributed==2023.8.1\n",
            "distro==1.7.0\n",
            "dlib==19.24.4\n",
            "dm-tree==0.1.8\n",
            "docstring_parser==0.16\n",
            "docutils==0.18.1\n",
            "dopamine_rl==4.0.9\n",
            "duckdb==0.10.3\n",
            "earthengine-api==0.1.412\n",
            "easydict==1.13\n",
            "ecos==2.0.14\n",
            "editdistance==0.6.2\n",
            "eerepr==0.0.4\n",
            "en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl#sha256=86cc141f63942d4b2c5fcee06630fd6f904788d2f0ab005cce45aadb8fb73889\n",
            "entrypoints==0.4\n",
            "et-xmlfile==1.1.0\n",
            "etils==1.7.0\n",
            "etuples==0.3.9\n",
            "exceptiongroup==1.2.2\n",
            "fastai==2.7.15\n",
            "fastcore==1.5.54\n",
            "fastdownload==0.0.7\n",
            "fastjsonschema==2.20.0\n",
            "fastprogress==1.0.3\n",
            "fastrlock==0.8.2\n",
            "filelock==3.15.4\n",
            "fiona==1.9.6\n",
            "firebase-admin==5.3.0\n",
            "Flask==2.2.5\n",
            "flatbuffers==24.3.25\n",
            "flax==0.8.4\n",
            "folium==0.14.0\n",
            "fonttools==4.53.1\n",
            "frozendict==2.4.4\n",
            "frozenlist==1.4.1\n",
            "fsspec==2023.6.0\n",
            "future==0.18.3\n",
            "gast==0.6.0\n",
            "gcsfs==2023.6.0\n",
            "GDAL==3.6.4\n",
            "gdown==5.1.0\n",
            "geemap==0.33.1\n",
            "gensim==4.3.2\n",
            "geocoder==1.38.1\n",
            "geographiclib==2.0\n",
            "geopandas==0.13.2\n",
            "geopy==2.3.0\n",
            "gin-config==0.5.0\n",
            "glob2==0.7\n",
            "google==2.0.3\n",
            "google-ai-generativelanguage==0.6.6\n",
            "google-api-core==2.19.1\n",
            "google-api-python-client==2.137.0\n",
            "google-auth==2.27.0\n",
            "google-auth-httplib2==0.2.0\n",
            "google-auth-oauthlib==1.2.1\n",
            "google-cloud-aiplatform==1.59.0\n",
            "google-cloud-bigquery==3.25.0\n",
            "google-cloud-bigquery-connection==1.15.4\n",
            "google-cloud-bigquery-storage==2.25.0\n",
            "google-cloud-bigtable==2.24.0\n",
            "google-cloud-core==2.4.1\n",
            "google-cloud-datastore==2.19.0\n",
            "google-cloud-firestore==2.16.1\n",
            "google-cloud-functions==1.16.4\n",
            "google-cloud-iam==2.15.1\n",
            "google-cloud-language==2.13.4\n",
            "google-cloud-pubsub==2.22.0\n",
            "google-cloud-resource-manager==1.12.4\n",
            "google-cloud-storage==2.8.0\n",
            "google-cloud-translate==3.15.4\n",
            "google-colab @ file:///colabtools/dist/google-colab-1.0.0.tar.gz#sha256=65b31ad680b7063c692f771ae49ae443b9b227ab591838f3be1133ab1ec3b088\n",
            "google-crc32c==1.5.0\n",
            "google-generativeai==0.7.2\n",
            "google-pasta==0.2.0\n",
            "google-resumable-media==2.7.1\n",
            "googleapis-common-protos==1.63.2\n",
            "googledrivedownloader==0.4\n",
            "graphviz==0.20.3\n",
            "greenlet==3.0.3\n",
            "grpc-google-iam-v1==0.13.1\n",
            "grpcio==1.64.1\n",
            "grpcio-status==1.48.2\n",
            "gspread==6.0.2\n",
            "gspread-dataframe==3.3.1\n",
            "gym==0.25.2\n",
            "gym-notices==0.0.8\n",
            "h5netcdf==1.3.0\n",
            "h5py==3.9.0\n",
            "holidays==0.53\n",
            "holoviews==1.17.1\n",
            "html5lib==1.1\n",
            "httpimport==1.3.1\n",
            "httplib2==0.22.0\n",
            "huggingface-hub==0.23.5\n",
            "humanize==4.7.0\n",
            "hyperopt==0.2.7\n",
            "ibis-framework==8.0.0\n",
            "idna==3.7\n",
            "imageio==2.31.6\n",
            "imageio-ffmpeg==0.5.1\n",
            "imagesize==1.4.1\n",
            "imbalanced-learn==0.10.1\n",
            "imgaug==0.4.0\n",
            "immutabledict==4.2.0\n",
            "importlib_metadata==8.0.0\n",
            "importlib_resources==6.4.0\n",
            "imutils==0.5.4\n",
            "inflect==7.0.0\n",
            "iniconfig==2.0.0\n",
            "intel-openmp==2023.2.4\n",
            "ipyevents==2.0.2\n",
            "ipyfilechooser==0.6.0\n",
            "ipykernel==5.5.6\n",
            "ipyleaflet==0.18.2\n",
            "ipyparallel==8.8.0\n",
            "ipython==7.34.0\n",
            "ipython-genutils==0.2.0\n",
            "ipython-sql==0.5.0\n",
            "ipytree==0.2.2\n",
            "ipywidgets==7.7.1\n",
            "itsdangerous==2.2.0\n",
            "jax==0.4.26\n",
            "jaxlib @ https://storage.googleapis.com/jax-releases/cuda12/jaxlib-0.4.26+cuda12.cudnn89-cp310-cp310-manylinux2014_x86_64.whl#sha256=813cf1fe3e7ca4dbf5327d6e7b4fc8521e92d8bba073ee645ae0d5d036a25750\n",
            "jeepney==0.7.1\n",
            "jellyfish==1.0.4\n",
            "jieba==0.42.1\n",
            "Jinja2==3.1.4\n",
            "joblib==1.4.2\n",
            "jsonpickle==3.2.2\n",
            "jsonschema==4.19.2\n",
            "jsonschema-specifications==2023.12.1\n",
            "jupyter-client==6.1.12\n",
            "jupyter-console==6.1.0\n",
            "jupyter-server==1.24.0\n",
            "jupyter_core==5.7.2\n",
            "jupyterlab_pygments==0.3.0\n",
            "jupyterlab_widgets==3.0.11\n",
            "kaggle==1.6.14\n",
            "kagglehub==0.2.8\n",
            "keras==2.15.0\n",
            "keyring==23.5.0\n",
            "kiwisolver==1.4.5\n",
            "langcodes==3.4.0\n",
            "language_data==1.2.0\n",
            "launchpadlib==1.10.16\n",
            "lazr.restfulclient==0.14.4\n",
            "lazr.uri==1.0.6\n",
            "lazy_loader==0.4\n",
            "libclang==18.1.1\n",
            "librosa==0.10.2.post1\n",
            "lightgbm==4.1.0\n",
            "linkify-it-py==2.0.3\n",
            "llvmlite==0.41.1\n",
            "locket==1.0.0\n",
            "logical-unification==0.4.6\n",
            "lxml==4.9.4\n",
            "malloy==2023.1067\n",
            "marisa-trie==1.2.0\n",
            "Markdown==3.6\n",
            "markdown-it-py==3.0.0\n",
            "MarkupSafe==2.1.5\n",
            "matplotlib==3.7.1\n",
            "matplotlib-inline==0.1.7\n",
            "matplotlib-venn==0.11.10\n",
            "mdit-py-plugins==0.4.1\n",
            "mdurl==0.1.2\n",
            "miniKanren==1.0.3\n",
            "missingno==0.5.2\n",
            "mistune==0.8.4\n",
            "mizani==0.9.3\n",
            "mkl==2023.2.0\n",
            "ml-dtypes==0.2.0\n",
            "mlxtend==0.22.0\n",
            "more-itertools==10.1.0\n",
            "moviepy==1.0.3\n",
            "mpmath==1.3.0\n",
            "msgpack==1.0.8\n",
            "multidict==6.0.5\n",
            "multipledispatch==1.0.0\n",
            "multitasking==0.0.11\n",
            "murmurhash==1.0.10\n",
            "music21==9.1.0\n",
            "natsort==8.4.0\n",
            "nbclassic==1.1.0\n",
            "nbclient==0.10.0\n",
            "nbconvert==6.5.4\n",
            "nbformat==5.10.4\n",
            "nest-asyncio==1.6.0\n",
            "networkx==3.3\n",
            "nibabel==4.0.2\n",
            "nltk==3.8.1\n",
            "notebook==6.5.5\n",
            "notebook_shim==0.2.4\n",
            "numba==0.58.1\n",
            "numexpr==2.10.1\n",
            "numpy==1.25.2\n",
            "nvidia-cublas-cu12==12.1.3.1\n",
            "nvidia-cuda-cupti-cu12==12.1.105\n",
            "nvidia-cuda-nvrtc-cu12==12.1.105\n",
            "nvidia-cuda-runtime-cu12==12.1.105\n",
            "nvidia-cudnn-cu12==8.9.2.26\n",
            "nvidia-cufft-cu12==11.0.2.54\n",
            "nvidia-curand-cu12==10.3.2.106\n",
            "nvidia-cusolver-cu12==11.4.5.107\n",
            "nvidia-cusparse-cu12==12.1.0.106\n",
            "nvidia-nccl-cu12==2.20.5\n",
            "nvidia-nvjitlink-cu12==12.5.82\n",
            "nvidia-nvtx-cu12==12.1.105\n",
            "nvtx==0.2.10\n",
            "oauth2client==4.1.3\n",
            "oauthlib==3.2.2\n",
            "onnx==1.16.1\n",
            "onnx-graphsurgeon==0.5.2\n",
            "opencv-contrib-python==4.8.0.76\n",
            "opencv-python==4.8.0.76\n",
            "opencv-python-headless==4.10.0.84\n",
            "openpyxl==3.1.5\n",
            "opt-einsum==3.3.0\n",
            "optax==0.2.2\n",
            "orbax-checkpoint==0.4.4\n",
            "osqp==0.6.2.post8\n",
            "packaging==24.1\n",
            "pandas==2.0.3\n",
            "pandas-datareader==0.10.0\n",
            "pandas-gbq==0.19.2\n",
            "pandas-stubs==2.0.3.230814\n",
            "pandocfilters==1.5.1\n",
            "panel==1.3.8\n",
            "param==2.1.1\n",
            "parso==0.8.4\n",
            "parsy==2.1\n",
            "partd==1.4.2\n",
            "pathlib==1.0.1\n",
            "patsy==0.5.6\n",
            "peewee==3.17.6\n",
            "pexpect==4.9.0\n",
            "pickleshare==0.7.5\n",
            "Pillow==9.4.0\n",
            "pip-tools==6.13.0\n",
            "platformdirs==4.2.2\n",
            "plotly==5.15.0\n",
            "plotnine==0.12.4\n",
            "pluggy==1.5.0\n",
            "polars==0.20.2\n",
            "pooch==1.8.2\n",
            "portpicker==1.5.2\n",
            "prefetch-generator==1.0.3\n",
            "preshed==3.0.9\n",
            "prettytable==3.10.2\n",
            "proglog==0.1.10\n",
            "progressbar2==4.2.0\n",
            "prometheus_client==0.20.0\n",
            "promise==2.3\n",
            "prompt_toolkit==3.0.47\n",
            "prophet==1.1.5\n",
            "proto-plus==1.24.0\n",
            "protobuf==3.20.3\n",
            "psutil==5.9.5\n",
            "psycopg2==2.9.9\n",
            "ptyprocess==0.7.0\n",
            "py-cpuinfo==9.0.0\n",
            "py4j==0.10.9.7\n",
            "pyarrow==14.0.2\n",
            "pyarrow-hotfix==0.6\n",
            "pyasn1==0.6.0\n",
            "pyasn1_modules==0.4.0\n",
            "pycocotools==2.0.8\n",
            "pycparser==2.22\n",
            "pydantic==2.8.2\n",
            "pydantic_core==2.20.1\n",
            "pydata-google-auth==1.8.2\n",
            "pydot==1.4.2\n",
            "pydot-ng==2.0.0\n",
            "pydotplus==2.0.2\n",
            "PyDrive==1.3.1\n",
            "PyDrive2==1.6.3\n",
            "pyerfa==2.0.1.4\n",
            "pygame==2.6.0\n",
            "Pygments==2.16.1\n",
            "PyGObject==3.42.1\n",
            "PyJWT==2.3.0\n",
            "pymc==5.10.4\n",
            "pymystem3==0.2.0\n",
            "pynvjitlink-cu12==0.3.0\n",
            "PyOpenGL==3.1.7\n",
            "pyOpenSSL==24.1.0\n",
            "pyparsing==3.1.2\n",
            "pyperclip==1.9.0\n",
            "pyproj==3.6.1\n",
            "pyproject_hooks==1.1.0\n",
            "pyshp==2.3.1\n",
            "PySocks==1.7.1\n",
            "pytensor==2.18.6\n",
            "pytest==7.4.4\n",
            "python-apt==2.4.0\n",
            "python-box==7.2.0\n",
            "python-dateutil==2.8.2\n",
            "python-louvain==0.16\n",
            "python-slugify==8.0.4\n",
            "python-utils==3.8.2\n",
            "pytz==2023.4\n",
            "pyviz_comms==3.0.2\n",
            "PyWavelets==1.6.0\n",
            "PyYAML==6.0.1\n",
            "pyzmq==24.0.1\n",
            "qdldl==0.1.7.post4\n",
            "qudida==0.0.4\n",
            "ratelim==0.1.6\n",
            "referencing==0.35.1\n",
            "regex==2024.5.15\n",
            "requests==2.31.0\n",
            "requests-oauthlib==1.3.1\n",
            "requirements-parser==0.9.0\n",
            "rich==13.7.1\n",
            "rmm-cu12==24.4.0\n",
            "rpds-py==0.19.0\n",
            "rpy2==3.4.2\n",
            "rsa==4.9\n",
            "safetensors==0.4.3\n",
            "scikit-image==0.19.3\n",
            "scikit-learn==1.2.2\n",
            "scipy==1.11.4\n",
            "scooby==0.10.0\n",
            "scs==3.2.6\n",
            "seaborn==0.13.1\n",
            "SecretStorage==3.3.1\n",
            "Send2Trash==1.8.3\n",
            "sentencepiece==0.1.99\n",
            "shapely==2.0.5\n",
            "shellingham==1.5.4\n",
            "simple_parsing==0.1.5\n",
            "six==1.16.0\n",
            "sklearn-pandas==2.2.0\n",
            "smart-open==7.0.4\n",
            "sniffio==1.3.1\n",
            "snowballstemmer==2.2.0\n",
            "sortedcontainers==2.4.0\n",
            "soundfile==0.12.1\n",
            "soupsieve==2.5\n",
            "soxr==0.3.7\n",
            "spacy==3.7.5\n",
            "spacy-legacy==3.0.12\n",
            "spacy-loggers==1.0.5\n",
            "Sphinx==5.0.2\n",
            "sphinxcontrib-applehelp==1.0.8\n",
            "sphinxcontrib-devhelp==1.0.6\n",
            "sphinxcontrib-htmlhelp==2.0.5\n",
            "sphinxcontrib-jsmath==1.0.1\n",
            "sphinxcontrib-qthelp==1.0.7\n",
            "sphinxcontrib-serializinghtml==1.1.10\n",
            "SQLAlchemy==2.0.31\n",
            "sqlglot==20.11.0\n",
            "sqlparse==0.5.1\n",
            "srsly==2.4.8\n",
            "stanio==0.5.1\n",
            "statsmodels==0.14.2\n",
            "StrEnum==0.4.15\n",
            "sympy==1.13.0\n",
            "tables==3.8.0\n",
            "tabulate==0.9.0\n",
            "tbb==2021.13.0\n",
            "tblib==3.0.0\n",
            "tenacity==8.5.0\n",
            "tensorboard==2.15.2\n",
            "tensorboard-data-server==0.7.2\n",
            "tensorflow @ https://storage.googleapis.com/colab-tf-builds-public-09h6ksrfwbb9g9xv/tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl#sha256=a2ec79931350b378c1ef300ca836b52a55751acb71a433582508a07f0de57c42\n",
            "tensorflow-datasets==4.9.6\n",
            "tensorflow-estimator==2.15.0\n",
            "tensorflow-gcs-config==2.15.0\n",
            "tensorflow-hub==0.16.1\n",
            "tensorflow-io-gcs-filesystem==0.37.1\n",
            "tensorflow-metadata==1.15.0\n",
            "tensorflow-probability==0.23.0\n",
            "tensorrt==10.2.0.post1\n",
            "tensorrt-cu12==10.2.0.post1\n",
            "tensorrt-cu12-bindings==10.2.0.post1\n",
            "tensorrt-cu12-libs==10.2.0.post1\n",
            "tensorstore==0.1.45\n",
            "termcolor==2.4.0\n",
            "terminado==0.18.1\n",
            "text-unidecode==1.3\n",
            "textblob==0.17.1\n",
            "tf-slim==1.1.0\n",
            "tf_keras==2.15.1\n",
            "thinc==8.2.5\n",
            "threadpoolctl==3.5.0\n",
            "tifffile==2024.7.2\n",
            "tinycss2==1.3.0\n",
            "tokenizers==0.19.1\n",
            "toml==0.10.2\n",
            "tomli==2.0.1\n",
            "toolz==0.12.1\n",
            "torch @ https://download.pytorch.org/whl/cu121/torch-2.3.1%2Bcu121-cp310-cp310-linux_x86_64.whl#sha256=f0deb5d2f932a68ed54625ba140eddbf2af22be978ee19b9b63c986add6425b2\n",
            "torchaudio @ https://download.pytorch.org/whl/cu121/torchaudio-2.3.1%2Bcu121-cp310-cp310-linux_x86_64.whl#sha256=0b423f4ae3356f11f6723e8c77208ac3f9361a4f941e4cc08d86c32c137594bc\n",
            "torchsummary==1.5.1\n",
            "torchtext==0.18.0\n",
            "torchvision @ https://download.pytorch.org/whl/cu121/torchvision-0.18.1%2Bcu121-cp310-cp310-linux_x86_64.whl#sha256=e95ba5a2c616939281e01babf11664d6d1725e81bba57ef81f81c3e57e4d4151\n",
            "tornado==6.3.3\n",
            "tqdm==4.66.4\n",
            "traitlets==5.7.1\n",
            "traittypes==0.2.1\n",
            "transformers==4.42.4\n",
            "triton==2.3.1\n",
            "tweepy==4.14.0\n",
            "typer==0.12.3\n",
            "types-pytz==2024.1.0.20240417\n",
            "types-setuptools==70.3.0.20240710\n",
            "typing_extensions==4.12.2\n",
            "tzdata==2024.1\n",
            "tzlocal==5.2\n",
            "uc-micro-py==1.0.3\n",
            "uritemplate==4.1.1\n",
            "urllib3==2.0.7\n",
            "vega-datasets==0.9.0\n",
            "wadllib==1.3.6\n",
            "wasabi==1.1.3\n",
            "wcwidth==0.2.13\n",
            "weasel==0.4.1\n",
            "webcolors==24.6.0\n",
            "webencodings==0.5.1\n",
            "websocket-client==1.8.0\n",
            "Werkzeug==3.0.3\n",
            "widgetsnbextension==3.6.7\n",
            "wordcloud==1.9.3\n",
            "wrapt==1.14.1\n",
            "xarray==2023.7.0\n",
            "xarray-einstats==0.7.0\n",
            "xgboost==2.0.3\n",
            "xlrd==2.0.1\n",
            "xyzservices==2024.6.0\n",
            "yarl==1.9.4\n",
            "yellowbrick==1.5\n",
            "yfinance==0.2.40\n",
            "zict==3.0.0\n",
            "zipp==3.19.2\n"
          ]
        }
      ]
    }
  ]
}